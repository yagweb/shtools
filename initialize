#install OS
	#remove the partitions of SSD, recreate for root and swap.

#network
	ifconfig | grep HWaddr
	ethtool ethXXX
	#update /etc/network/interfaces file, or use NetworkManager

#copy shtools, modify file .rcbash
	#export PATH="/home/XXX/shtools:$PATH"

	#modified file proxy.ini

	#usage
	. proxy
	. proxy apt-on
	. proxy apt-nocache-on

#vi up-down become ABCD issue
	sudo apt-get remove vim-common
	sudo apt-get install vim

#ssh
	apt-get install ssh-server
	ps -el | grep ssh  #check
	sudo /etc/init.d/ssh start

	Setup passphraseless ssh - master deamons use ssh to control slaves
		ssh-keygen -t rsa -P ""
		#hadoop services on the same machine need comunication via ssh!!
		cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys
		chmod 0600 ~/.ssh/authorized_keys

		#test it, then exit
		ssh localhost 
		#debug
		ssh -vvv localhost

		#named node, yarn node, copy its shh-key to other nodes
		ssh-copy-id user@host
		or
		ssh user@host 'mkdir -p .ssh&& cat >> .ssh/authorized_keys' < ~/.ssh/id_rsa.pub

#git
	apt-get install git
	ssh-keygen

#xrdp + mate
    #if not use mate ubuntu
    echo "mate-session">~/.xsession
    
	apt-get install xrdp
	sudo vi /etc/xrdp/startwm.sh
	#append
	  . /etc/environment
	  . /etc/profile

	sudo vi /etc/xrdp/xrdp.ini
	#insert
	  [xrdp1]
	  name=vnc-exists
	  lib=libvnc.so
	  username=askXXX
	  password=ask
	  ip=127.0.0.1
	  port=ask5910

	sudo service xrdp restart 

# now we can use Remote Desktop
# mate has Pluma, Pluma is based on gedit, So we don't need to install gedit.

#/etx/fstab
	#mount local device and nfs
	sudo pluma /etc/fstab

	#for local
	/dev/sdcX   /mnt/XX      ext4    defaults        0       2
    #for nfs. path must be in /etc/exports of machine ip
	ip:path     /mnt/002c    nfs     defaults        0       2

	sudo mount -a

    #check
    sudo df
    sudo mount -l

#samba
	sudo apt-get install samba
    #usually I will add two users, root the current user
	sudo smbpasswd -a XXXX
	sudo pluma /etc/samba/smb.conf

	#append:
	  [root]
		comment = root
		path = /
		browseable = yes
		read only = no
		valid users = XX, @group

    #test
    testparm

	sudo /etc/init.d/samba restart
    #sudo service smbd restart

#nfs
	#on both client and server linux machine,
	sudo apt-get install nfs-kernel-server
	sudo service nfs-kernel-server start

    #server setting
    sudo vi /etc/exports
       #no blank follow the comma!!!!
      <loacalpath> *(insecure,rw,sync,no_subtree_check,no_root_squash)
    #reload config file
    sudo exportfs -avr
    service nfs-kernel-server reload 

    #client setting
    sudo mount -t nfs ip:<path>  <localpath>

    use ip = localhost, to mount test

#cifs
	sudo apt-get install cifs-utils
	#sudo mkdir <local_path>
	#sudo mount -t cifs -o username=XXX,password=XXX,uid=XXX //winIP/folder/ <local_path>

#application softwares
	(runfile should use 'bash/sh' to install, not '.')

    gcc

	anaconda 
		#spyder cannot input bug, XKEYBOARD extension not present on the X server
        echo "export QT_XKB_CONFIG_ROOT=/usr/share/X11/xkb" >> .bashrc
        source .bashrc

		pip install virtualenv
		pip install jupyterlab    #a web-based IDE !!! jupyter lab to start it!
			#edit config script to disable login token
			jupyter lab --generate-config
				c.NotebookApp.token = '<generated>'
			#in project folder,
			current="$(cd "$( dirname "${BASH_SOURCE[0]}" )" && pwd )"
			jupyter lab --notebook-dir=$current --ip=0.0.0.0 --no-browser

	cmake, blas,..
	mysql
	FAST
    CUDA
		"you appear to be running an X server"
        ctrl+alt+F1  #switch to tty1 user mode, GUI is running backend
        
		#close X server service
        sudo /etc/init.d/XXdm stop
        #XXdm: lightdm for Unity, kdm for Kubuntu, gdm for gnome, 
        #check, ctrl+alt+F7, shift back to GUI, should not show any desktop now
		
		#xrdp remote desktop could not be closed by stop xrdp service!
        #use my vnckill script!!
        #stop the xrdp in case some one log in during the installation
        sudo service xrdp stop

		#check, 
        #ls -a /tmp/.X*

		sudo service xrdp start
		sudo /etc/init.d/XXXX start

		runfile, Run 'sudo sh XXXX.run  --silence'
        sudo reboot
        
		echo "export LD_LIBRARY_PATH=/usr/local/cuda/lib64/:$LD_LIBRARY_PATH" >> .bashrc
		echo "export PATH=/usr/local/cuda/bin/:$PATH" >> .bashrc

	cuDNN
		#file can be copied to any place, here we put it to cuda
		tar -xvf ...
		sudo cp <the file cudnn.h> /usr/local/cuda/include
        sudo cp <the files libcudnn*> /usr/local/cuda/lib64
		sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*
   		append CuDNN path to CUDA_HOME

	TensorFlow #better installed in virtualenv,
		pip install --upgrade pip
		pip install --upgrade --ignore-installed setuptools
		sudo apt-get install --upgrade tensorflow-gpu
        
		#test
		import tensorflow as tf
		print(tf.__version__)

	PyTorch

	Jave
		1 sudo tar -xvf <java.gz> -C /usr/lib/jvm
		2 edit the .bashrc of hduser
			export JAVA_HOME=/usr/lib/jvm/<jdk1.7.0_80>
			export JRE_HOME=${JAVA_HOME}/jre
			export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib
			export PATH=${JAVA_HOME}/bin:$PATH
		3 set the default java
			#if the user doesnot export ,
			#export JAVA_HOME=/usr/lib/jvm/<jdk1.7.0_80>
			else, load it first
			source .bashrc

			exec the set-java script use . not sh!!				
			#test, java -version, javac -version

	install pdsh
		#all the nodes should bi-passphraseless ssh with the pdsh machine, include itself
		pdsh -V #test
		pdsh -w <hosts, > command
        pdsh -g <file> command

	Hadoop & Spark
		sudo addgroup hadoop  #confirm by /etc/group
		#add folder under home, orelse use useradd   
		sudo adduser --ingroup hadoop hduser
  
		#we may need to run some sudo commands
		sudo visudo  #add root privilege for hadoop group
		#pluma /etc/sudoers  #directly edit the file may cause error
		#for a group, the line hould start with %   
        ###for user or sudo pluma /etc/passwd  #update UID to 0
        #login with the hadoop user

		#update /etc/hosts : copy to outer machines directly
		ip phm1
		ip phm2
		...
		#ping test
		ping phmX

		Setup passphraseless ssh - master deamons use ssh to control slaves
			ssh-keygen -t rsa -P ""
			#hadoop services on the same machine need comunication via ssh!!
			cat /home/hadoop/.ssh/id_rsa.pub >> /home/hadoop/.ssh/authorized_keys
			chmod 0600 ~/.ssh/authorized_keys

			#test it, then exit
			ssh localhost 
			#debug
			ssh -vvv localhost

			#named node, yarn node, copy its shh-key to other nodes
			ssh-copy-id user@host  (user can be omitted if it's same with the current user)
			or
			ssh user@host 'mkdir -p .ssh&& cat >> .ssh/authorized_keys' < ~/.ssh/id_rsa.pub

		#create folder /home/hadoop, chown!!
		sudo mkdir /home/hadoop #suggested
		sudo chown -R hduser:hadoop /home/hadoop
		#create some temp - keep the same structure on all machine! use pdsh

		#add all hosts of hadoop cluster to new file ~/.dsh/group/hadoop
		#-g is not working
		#the default user used by pdsh is pdsh, not the current user!
		pdsh -w ssh:hduser@phm[1-3] mkdir 
					/home/hadoop/
						dfs
		                   dfs/name
		                   dfs/data
						temp
						   temp/dfs
						yarn
						   yarn/node
						   yarn/app-logs
						   yarn/log

		download java, scala, hadoop, spark; version should compatiable
		1 Install Java

		#if firwall is running, disable it or enable port
		sudo ufw status
		sudo ufw disable

		2 Install Hadoop (under any user, chown to hduser)

			#we can config in the pdsh machine, then copy the whole source folder to other machines
			copy config files to other machine
				pdsh -w ssh:hduser@phm[2-3] scp phm1:$HADOOP_HOME $HADOOP_HOME/..

			#we can also copy any file as we need. such as
			copy config files to other machine
				pdsh -w ssh:hduser@phm[2-3] scp phm1:$HADOOP_ETC/hadoop-env.sh $HADOOP_ETC

			tar -xvf XX/hadoop-X.X.X.tar.gz -C /home/hadoop
			edit env variables .bashrc: to simplify the command call
				export HADOOP_HOME=/home/hadoop/hadoop-X.X.X
				export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH
				#only the pdsh machine need this variable
				export HADOOP_CONF_DIR=/home/hadoop/hadoop-2.3.7/etc/hadoop

			NameNode, ResourceManager, MR Job History,.. : rest DataNode and NodeManager
			use different account for deamons, such as, 'hdfs' for HDFS processes, 'yarn' account for YARN

			config files in etc/hodoop/ folder, mainly 2 sh files (store environment Variable) and 4 xmlfiles
			  for xml file, add,
				<property> <name></name>  <value></value> </property>

				hadoop-env.sh
					export JAVA_HOME=/usr/lib/jvm/jdk1.7.0_80
					#other parameters, HADOOP_PID_DIR, HADOOP_LOG_DIR
					#export HADOOP_OPTS=-Djava.net.preferIPV4Stack=true

				core-site.xml
					#can set <namenode> = localhost when install in single machine for test
					#cluster install be replaced with namenode's ip
					#use hostname not ip, edit /etc/hosts

					#let Hadoop modules konw where the HDFS NameNode is located
                    fs.defaultFS            hdfs://<host-of-namenode>:9000

					#io.file.buffer.size          131072

					#can be deleted for datanode recorvering, outside the hadoop home folder
					#we need to copy folder to other places
					hadoop.tmp.dir                   /home/hadoop/temp
					hadoop.proxyuser.hduser.hosts    *
					hadoop.proxyuser.hduser.groups   *

			hdfs services [NameNode, DataNode, etc.]

				hdfs-site.xml
				  	all items, dfs.hosts.XX dfs.namenode.XX dfs.datanode.XX

					dfs.replication              1 #less than the number of datanodes
					dfs.namenode.http-address    <ip-of-out-net>:50070					
					dfs.namenode.name.dir        file:///home/hadoop/dfs/name
					#dfs.datanode.http.address    localhost:50075
					dfs.datanode.data.dir        file:///home/hadoop/dfs/data, XXX #other disk folders!!					
					dfs.namenode.secondary.http-address   phm1:9001					
					dfs.webhdfs.enabled          true

				copy-to-slaves by pdsh
					
			    start  :  namenode, datanode, secondarynamenode
					#if $HADOOP_HOME/bin/ and $HADOOP_HOME/sbin/ are added to PATH, 
					#we can use commands or scripts name such as 'hdfs' directly, else we need '$HADOOP_HOME/bin/hdfs'

					#For the first time, format a new distributed filesystem as hdfs
					hdfs namenode -format  [<cluster_name>] 

					#or, start the processes on all the nodes
					#do it on the namenode!
					start-dfs.sh
					
					#start one by one as needed, pdsh can be used 
					hadoop-daemon.sh start namenode
					hadoop-daemon.sh start secondarynamenode
					#use pdsh to start all datanode, pdsh -w ssh:hduser@phm[2-3] $HADOOP_HOME/sbin/  
					#pdsh must use fullpath of hadoop-daemon, because .bashrc is not loaded by ssh process?
					hadoop-daemon.sh start datanode

				check 
					#check whether the daemons are running:
					#jps is a java command, java VM process status tool 
					#same as, ps aux | grep java
					pdsh -w ssh:hduser@phm[1-3] $JAVA_HOME/bin/jps

					#then check the web interface of datanode   
					http://<hostname>:50070
                    #datanode
					http://<hostname>:50075
					#secondarynamenode (in the config file)
					phm1:9001	

				stop
					stop-dfs.sh
					or, stop one by one,
					hadoop-daemon.sh stop XX

			yarn services

				#not needed? because it added in .bashrc
				yarn-env.sh
					export JAVA_HOME=/usr/lib/jvm/jdk1.7.0_80

				yarn-site.xml for NodeManager, ResourceManager and HistoryManager etc.
				  all items:
					yarn.acl.XX, 
					yarn.resourcemanager.XXX
					yarn.schedular.XXX
					yarn.nodemanager.XXX
					yarn.log-aggregation.XX
				  for single node
					yarn.nodemanager.aux-services    mapreduce_shuffle
					yarn.nodemanager.env-whitelist   JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME
				  for cluster
					yarn.resourcemanager.hostname                      phm1
					yarn.resourcemanager.resource-tracker.address      phm1:8031
					yarn.resourcemanager.scheduler.address             phm1:8030
					yarn.resourcemanager.scheduler.class               org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler
					yarn.resourcemanager.address                       phm1:8032 
					yarn.resourcemanager.admin.address                 phm1:8033
					yarn.resourcemanager.webapp.address                <ip-of-out-net>:8088  #can be bind to the other network interface
					yarn.nodemanager.local-dirs                        /home/hadoop/yarn/node
					yarn.nodemanager.address                           localhost:8994   #localhot, not master!!
					yarn.nodemanager.resource.memory-mb                40960 # must large than 1024
                    yarn.nodemanager.resource.cpu-vcores               56 # cat /proc/cpuinfo | grep "processor"
					yarn.nodemanager.remote-app-log-dir                /home/hadoop/yarn/app-logs
					yarn.nodemanager.log-dirs                          /home/hadoop/yarn/log
					yarn.nodemanager.aux-services                      mapreduce_shuffle
					yarn.nodemanager.aux-services.mapreduce.shuffle.class   org.apache.hadoop.mapred.ShuffleHandler

				slaves  #used to send command to all these nodes
					list all hostname/ip in /etc/hadoop/works file

			start yarn services : resourcemanager, nodemanager, proxymanager

				#on the center machine, all the workers in file 'workers' will be started!
				start-yarn.sh 

				#one by one as needed, according to each machine's role:
				yarn-deamon.sh start resourcemanager
				yarn-deamon.sh start nodemanager
				yarn-deamon.sh start proxymanager

			check  # web must use phm1, not localhost, if you have 2 or more net interfaces!!
				#check whethere the processes are running via jps
				jps #list the java process on this machine, hdfs and yarn are all java ps

				#then,
				#web interface for ResourceManager: 
				http://phm1:8088   #yarn.resourcemanager.webapp.address in yarn-site.xml

				#web interface for mapred:
			    http://phm1:19888

			check log! if server cannot start!
				$HADOOP_HOME/logs

			stop
				stop-yarn.sh
				#one-by-one
				yarn-deamon.sh stop XXX 

			mapred module : [optional jobhistory service]
				#it can run locally, and run on yarn; it can use local fs, or hdfs!!

				#default parameters - user can change set these parameters for each job!!

				mapred-site.xml:
					#mapreduce.map.XX, mapreduce.reduce.XX, mapreduce.task.XX, mapreduce.jobhistory.XX
				    
					#if remove this line, MR will run locally
					mapreduce.framework.name         yarn
				
					#mapred run by nodemanager, write to jobhistory according to this config
					mapreduce.jobhistory.address            phm1:10020
					mapreduce.jobhistory.webapp.address     <ip-of-out-net>:19888
					#mapreduce.jobhistory.intermediate-done-dir    /home/hadoop/XX  #default is hdfs folder
					#mapreduce.jobhistory.done-dir          XXX

					mapreduce.job.tracker            phm1:54311

				start
					mr-jobhistory-daemon.sh start historyserver

		run example in local fs non-distributed mode - MapReduce job #open terminal at hadoop home
			mkdir input
			cp etc/hadoop/*.xml input				
			hadoop jar .share/hadoop/mapreduce share/hadoop/mapreduce/hadoop-mapreduce-examples-X.X.X.jar grep input output 'dfs[a-z.]+'
			cat output/*

		run example in hdfs
			#these two folder are the current working folder just like linux ~
			# for example the input means  /user/<username>/input
			# or you can absolute path, such as /input
			hdfs dfs -mkdir /user
			hdfs dfs -mkdir /user/<username>

			#cp data from local fs to hdfs
			hdfs dfs -mkdir input
			hdfs dfs -put $HADOOP_HOME/etc/hadoop/*.xml input

			hadoop jar .share/hadoop/mapreduce share/hadoop/mapreduce/hadoop-mapreduce-examples-X.X.X.jar grep input output 'dfs[a-z.]+'

			#cp data from hdfs to local fs
			hdfs dfs -get output output
			cat output/*
		FAQ:
			#nodemanager not running
			#nodemanager cann't allocate containers

	
